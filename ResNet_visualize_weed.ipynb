{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import h5py\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm_notebook\n",
    "import cv2\n",
    "\n",
    "print(torch.__version__)\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Weedread(Dataset):\n",
    "    def __init__(self, name, transform=None):\n",
    "        hf = h5py.File(name, 'r')\n",
    "        self.input_images = np.array(hf.get('data'), np.uint8)\n",
    "        self.target_labels = np.array(hf.get('labels')).astype(np.long)\n",
    "        self.transform = transform\n",
    "        hf.close()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.input_images.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        images = self.input_images[idx]\n",
    "        classes = self.target_labels[idx][1]\n",
    "        family =  self.target_labels[idx][0]\n",
    "        if self.transform is not None:\n",
    "            images = self.transform(images)\n",
    "        images = images\n",
    "        \n",
    "        return images, classes, family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_CHANNEL = 3\n",
    "BATCH_SIZE = 1\n",
    "normalize = transforms.Compose([\n",
    "    #transforms.ToPILImage(),\n",
    "    #transforms.Resize((96,96)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "classes = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \n",
    "           \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"21\"]\n",
    "\n",
    "data_path = os.path.dirname(os.getcwd()) + \"/data/weed/\"\n",
    "Train_data = Weedread(data_path + \"train.h5\", transform=normalize)\n",
    "Test_data = Weedread(data_path + \"val.h5\", transform=normalize)\n",
    "\n",
    "# Train_dataloader = DataLoader(dataset=Train_data,\n",
    "#                               batch_size = BATCH_SIZE,\n",
    "#                               shuffle=True)\n",
    "Test_dataloader = DataLoader(dataset=Test_data,\n",
    "                              batch_size = BATCH_SIZE,\n",
    "                              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "class My_Model(nn.Module):\n",
    "    def __init__(self, input_channel=1, num_class=21, num_family = 5):\n",
    "        super(My_Model, self).__init__()\n",
    "        model = models.resnet18(pretrained=True)\n",
    "        self.model_ft = torch.nn.Sequential(*(list(model.children())[:-1]))\n",
    "        set_parameter_requires_grad(self.model_ft, False)\n",
    "\n",
    "        self.family_fc = nn.Linear(512, num_family)\n",
    "        self.class_fc = nn.Linear(512, num_class)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Perform the usual forward pass\n",
    "        x = self.model_ft(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x_class = self.class_fc(x)\n",
    "        x_family = self.family_fc(x)\n",
    "        return F.softmax(x_class, dim=1), F.softmax(x_family, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "train_images, _, _ = next(iter(Test_dataloader))\n",
    "\n",
    "_model = My_Model(num_class=21, num_family = 5)\n",
    "_model.to(device)\n",
    "\n",
    "\n",
    "print(_model)\n",
    "#summary(_model, input_size= train_images[0].size())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "optimizer = torch.optim.SGD(_model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "EPOCHS = 100\n",
    "max_correct = 0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    #training\n",
    "    _model.train()\n",
    "    for image, label in tqdm_notebook(Train_dataloader):\n",
    "        image, label = image.to(device), label.to(device)\n",
    "        image = image.float()\n",
    "        output = _model(image)\n",
    "        #print(output)\n",
    "        loss = criterion(output, label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print('Loss :{:.4f} Epoch[{}/{}]'.format(loss.item(), epoch, EPOCHS))\n",
    "    #testing\n",
    "    _model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for image, label in (Test_dataloader):\n",
    "            image, label = image.to(device), label.to(device)\n",
    "            image = image.float()\n",
    "            outputs = _model(image)\n",
    "            predicted = torch.argmax(outputs,dim=1)\n",
    "            total += label.size(0)\n",
    "            correct += (predicted == label).sum().item()\n",
    "        print('Test Accuracy of the model on the test images: {:.4f} %'.format(100 * correct / total))\n",
    "    if(correct > max_correct):\n",
    "        max_correct = correct\n",
    "        torch.save(_model.state_dict(), 'epochs/ResNet18_cifar.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "inv_normalize = transforms.Normalize([-0.485/0.229, -0.456/0.224, -0.406/0.225], [1/0.229, 1/0.224, 1/0.225])\n",
    "\n",
    "def tensor_to_img(t):\n",
    "    \"\"\"Convert normalized tensor in Cuda to cv2 image\"\"\"\n",
    "    unnormalized = inv_normalize(t)\n",
    "    npimg = np.transpose(unnormalized.cpu().numpy(), (1, 2, 0))\n",
    "    return npimg\n",
    "\n",
    "def img_to_cuda_tensor(img):\n",
    "    tr_img = np.transpose(img, (2, 0, 1))\n",
    "    t = torch.from_numpy(tr_img)\n",
    "    t = normalize(t.float())\n",
    "    return t.to(device)\n",
    "    \n",
    "\n",
    "def imshow(img, title):\n",
    "    \"\"\"Custom function to display the image using matplotlib\"\"\"    \n",
    "    npimg = tensor_to_img(img)\n",
    "    #plot the numpy image\n",
    "    plt.figure(figsize = (4, 4))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(npimg)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveFeatures():\n",
    "    \"\"\"hook function at forward step\"\"\"\n",
    "    def __init__(self, module):\n",
    "        self.hook = module.register_forward_hook(self.hook_fn)\n",
    "    def hook_fn(self, module, input, output):\n",
    "        self.features = output\n",
    "    def close(self):\n",
    "        self.hook.remove()\n",
    "        \n",
    "class FilterVisualizer():\n",
    "    \"\"\"Visualize by maximize average activation\"\"\"\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        set_parameter_requires_grad(self.model, True)\n",
    "\n",
    "    def visualize(self, sz, layer, filter, upscaling_steps=10, upscaling_factor=1.2, lr=0.1, opt_steps=20, blur=None):\n",
    "        imarray = (np.random.random((sz, sz, 3)) * 20 + 128.)/255. #random tensor image\n",
    "        \n",
    "        activations = SaveFeatures(layer)  # register hook\n",
    "        for i in range(upscaling_steps):  # scale the image up upscaling_steps times\n",
    "            img = img_to_cuda_tensor(imarray)\n",
    "            img_var = Variable(img[None], requires_grad=True)  # convert image to Variable that requires grad\n",
    "            optimizer = torch.optim.Adam([img_var], lr=lr, weight_decay=1e-6)\n",
    "            if i > upscaling_steps/2:\n",
    "                opt_steps_ = int(opt_steps*1.3)\n",
    "            else:\n",
    "                opt_steps_ = opt_steps\n",
    "            for n in range(opt_steps_):  # optimize pixel values for opt_steps times\n",
    "                optimizer.zero_grad()\n",
    "                self.model(img_var)\n",
    "                loss = -1*activations.features[0, filter].mean()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            img = tensor_to_img(img_var.data[0])\n",
    "            self.output = img\n",
    "            sz = int(upscaling_factor * sz)  # calculate new image size\n",
    "            imarray = cv2.resize(img, (sz, sz), interpolation = cv2.INTER_CUBIC)  # scale image up\n",
    "            if blur is not None: img = cv2.blur(img,(blur,blur))  # blur image to reduce high frequency patterns\n",
    "                \n",
    "        activations.close()\n",
    "        return np.clip(self.output, 0, 1)\n",
    "        \n",
    "    def most_activated(self, image, layer, limit_top=None):\n",
    "        activations = SaveFeatures(layer)  # register hook\n",
    "        img = img_to_cuda_tensor(image)\n",
    "        self.model(Variable(img[0]))\n",
    "        \n",
    "        mean_act = [activations.features[0,i].mean().data.cpu().numpy()[0] for i in range(activations.features.shape[1])]\n",
    "        activations.close()\n",
    "        return mean_act\n",
    "\n",
    "def reconstructions_single_layer(FV, layer,layer_name,filters,\n",
    "                    init_size=56, upscaling_steps=12, upscaling_factor=1.2, \n",
    "                    opt_steps=20, blur=5, lr=1e-1,\n",
    "                    n_cols=5, cell_size=4,save_fig=False):\n",
    "    \n",
    "    imgs = []\n",
    "   \n",
    "    for i in range(len(filters)):\n",
    "        pic = FV.visualize(init_size,layer, filters[i], \n",
    "                                 upscaling_steps=upscaling_steps, \n",
    "                                 upscaling_factor=upscaling_factor, \n",
    "                                 opt_steps=opt_steps, blur=blur,\n",
    "                                 lr=lr)\n",
    "        imgs.append(pic)\n",
    "        \n",
    "    #plot feature maps\n",
    "    n_rows = ((len(imgs))//n_cols)\n",
    "    fig, axes = plt.subplots(n_rows,n_cols, figsize=(cell_size*n_cols,cell_size*n_rows))\n",
    "    for i,ax in enumerate(axes.flat):\n",
    "        ax.grid(False)\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        if i>=len(filters):\n",
    "            pass\n",
    "\n",
    "        ax.set_title(\"fmap \" + str(filters[i]))\n",
    "\n",
    "        ax.imshow(imgs[i])\n",
    "    fig.suptitle(layer_name, fontsize=\"x-large\",y=1.0)\n",
    "    plt.tight_layout()\n",
    "        \n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_model.load_state_dict(torch.load('epochs/ResNet18-base-line.pt'), strict=False)\n",
    "FV = FilterVisualizer(_model)\n",
    "pruned_model = nn.Sequential(*list(_model.model_ft.children()))\n",
    "l = [module for module in pruned_model.modules() \n",
    "     if (type(module) != nn.Sequential and type(module) != models.resnet.BasicBlock) ]\n",
    "\n",
    "print(len(l))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(l[0])\n",
    "imgs = reconstructions_single_layer(FV, l[0], \"\", list(range(0, 50)), lr = 0.01, opt_steps= 20, upscaling_steps = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(l[10])\n",
    "imgs = reconstructions_single_layer(FV, l[10], \"\", list(range(0, 50)), lr = 0.01, opt_steps= 20, upscaling_steps = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(l[20])\n",
    "imgs = reconstructions_single_layer(FV, l[20], \"\", list(range(0, 50)), lr = 0.01, opt_steps= 20, upscaling_steps = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(l[25])\n",
    "imgs = reconstructions_single_layer(FV, l[25], \"\", list(range(0, 50)), lr = 0.01, opt_steps= 20, upscaling_steps = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(l[30])\n",
    "imgs = reconstructions_single_layer(FV, l[30], \"\", list(range(0, 50)), lr = 0.01, opt_steps= 20, upscaling_steps = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(l[35])\n",
    "imgs = reconstructions_single_layer(FV, l[35], \"\", list(range(0, 50)), lr = 0.01, opt_steps= 20, upscaling_steps = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
