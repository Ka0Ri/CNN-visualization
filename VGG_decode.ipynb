{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import h5py\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm_notebook\n",
    "import cv2\n",
    "\n",
    "print(torch.__version__)\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "inv_normalize = transforms.Normalize([-0.485/0.229, -0.456/0.224, -0.406/0.225], [1/0.229, 1/0.224, 1/0.225])\n",
    "\n",
    "path = os.path.dirname(os.getcwd())\n",
    "trainset = torchvision.datasets.CIFAR10(root= path + '/data', train=True, download=True, transform=transform)\n",
    "Train_dataloader = DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)\n",
    "testset = torchvision.datasets.CIFAR10(root= path + '/data', train=False, download=True, transform=transform)\n",
    "Test_dataloader = DataLoader(testset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "class Vgg16Conv(nn.Module):\n",
    "    \"\"\"\n",
    "    vgg16 convolution network architecture\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channel=3, num_class=10):\n",
    "        super(Vgg16Conv, self).__init__()\n",
    "        model_ft = models.vgg16(pretrained=True)\n",
    "        set_parameter_requires_grad(model_ft, False) \n",
    "        #change FC\n",
    "        self.model_ft = nn.Sequential(*list(model_ft.features.children()))\n",
    "        self.FC = nn.Linear(8192, num_class)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Perform the usual forward pass\n",
    "        x = self.model_ft(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.FC(x)\n",
    "        return F.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vgg16Conv(\n",
       "  (model_ft): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (FC): Linear(in_features=8192, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "_model = Vgg16Conv(num_class = 10)\n",
    "_model.to(device)\n",
    "#summary(_model, input_size=(3, 128, 128))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "optimizer = torch.optim.SGD(_model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "EPOCHS = 100\n",
    "\n",
    "max_correct = 0\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    #training\n",
    "    _model.train()\n",
    "    for image, label in tqdm_notebook(Train_dataloader):\n",
    "        image, label = image.to(device), label.to(device)\n",
    "        image = image.float()\n",
    "        output = _model(image)\n",
    "        #print(output)\n",
    "        loss = criterion(output, label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print('Loss :{:.4f} Epoch[{}/{}]'.format(loss.item(), epoch, EPOCHS))\n",
    "    #testing\n",
    "    _model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for image, label in (Test_dataloader):\n",
    "            image, label = image.to(device), label.to(device)\n",
    "            image = image.float()\n",
    "            outputs = _model(image)\n",
    "            predicted = torch.argmax(outputs,dim=1)\n",
    "            total += label.size(0)\n",
    "            correct += (predicted == label).sum().item()\n",
    "        print('Test Accuracy of the model on the test images: {} %'.format(100 * correct / total))\n",
    "    if(correct > max_correct):\n",
    "        max_correct = correct\n",
    "        torch.save(_model.state_dict(), 'epochs/VGG_cifar.pt')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class VisualizeNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Deconv based on reference model\n",
    "    \"\"\"\n",
    "    def __init__(self, conv_model):\n",
    "        super(VisualizeNet, self).__init__()\n",
    "\n",
    "        reconstruction = []\n",
    "        construction = []\n",
    "        self.num_layers = 0\n",
    "        for idx, layer in conv_model.model_ft._modules.items():\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                reconstruction.append(nn.ConvTranspose2d(in_channels=layer.out_channels,\\\n",
    "                                                         out_channels=layer.in_channels,\\\n",
    "                                                         stride=layer.stride,\\\n",
    "                                                         kernel_size=layer.kernel_size,\\\n",
    "                                                         padding=layer.padding))\n",
    "                construction.append(nn.Conv2d(in_channels=layer.in_channels,\\\n",
    "                                                         out_channels=layer.out_channels,\\\n",
    "                                                         stride=layer.stride,\\\n",
    "                                                         kernel_size=layer.kernel_size,\\\n",
    "                                                         padding=layer.padding))\n",
    "                \n",
    "                reconstruction[-1].weight.data = layer.weight.data\n",
    "                construction[-1].weight.data = layer.weight.data\n",
    "            if isinstance(layer, nn.MaxPool2d):\n",
    "                reconstruction.append(nn.MaxUnpool2d(kernel_size=layer.kernel_size,\\\n",
    "                                                     stride=layer.stride,\\\n",
    "                                                     padding=layer.padding))\n",
    "                construction.append(nn.MaxPool2d(kernel_size=layer.kernel_size,\\\n",
    "                                                     stride=layer.stride,\\\n",
    "                                                     padding=layer.padding, return_indices=True))\n",
    "            if isinstance(layer, nn.ReLU):\n",
    "                reconstruction.append(nn.ReLU())\n",
    "                construction.append(nn.ReLU())\n",
    "            self.num_layers += 1\n",
    "            \n",
    "        self.rec = nn.Sequential(*reconstruction[::-1])\n",
    "        self.con = nn.Sequential(*construction)\n",
    "#         print(self.rec)\n",
    "#         print(self.con)\n",
    "        \n",
    "    def forward(self, x, vis_layer):\n",
    "        unpool2pool_indices = []\n",
    "        for idx, layer in self.con._modules.items():\n",
    "            print(layer)\n",
    "            if isinstance(layer, nn.MaxPool2d):\n",
    "                x, indice = layer(x)\n",
    "                unpool2pool_indices.append(indice)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "            if(vis_layer == int(idx)):\n",
    "                break\n",
    "        \n",
    "        num_feat = x.shape[1]\n",
    "        #mark zero other feature maps\n",
    "        last_layer = torch.zeros_like(x)\n",
    "        x_sum = torch.sum(x.view(1, num_feat, -1), dim=-1)\n",
    "        strong_activation_index = torch.argmax(x_sum, dim=1)\n",
    "        print(\"feature: \", strong_activation_index)\n",
    "        last_layer[:, strong_activation_index, : , :] = x[:, strong_activation_index, : , :]\n",
    "        \n",
    "        for idx, layer in self.rec._modules.items():\n",
    "            if(int(idx) >= self.num_layers - vis_layer - 1):\n",
    "                print(layer)\n",
    "                if isinstance(layer, nn.MaxUnpool2d):\n",
    "                    last_layer = layer(last_layer, unpool2pool_indices[-1])\n",
    "                    unpool2pool_indices.pop()\n",
    "                else:\n",
    "                    last_layer = layer(last_layer)\n",
    "        return F.relu(last_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_img(t):\n",
    "    \"\"\"Convert normalized tensor in Cuda to cv2 image\"\"\"\n",
    "    unnormalized = inv_normalize(t)\n",
    "    npimg = np.transpose(unnormalized.cpu().numpy(), (1, 2, 0))\n",
    "    return npimg\n",
    "\n",
    "def img_to_cuda_tensor(img):\n",
    "    tr_img = np.transpose(img, (2, 0, 1))\n",
    "    t = torch.from_numpy(tr_img)\n",
    "    t = normalize(t.float())\n",
    "    return t.to(device)\n",
    "    \n",
    "\n",
    "def imshow(img, title):\n",
    "    \"\"\"Custom function to display the image using matplotlib\"\"\"    \n",
    "    npimg = tensor_to_img(img)\n",
    "    #plot the numpy image\n",
    "    plt.figure(figsize = (4, 4))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(npimg)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
