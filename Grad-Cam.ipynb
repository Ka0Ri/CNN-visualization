{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchnet as tnt\n",
    "from torchnet.engine import Engine\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import h5py\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from torchsummary import summary\n",
    "\n",
    "from torch.nn.modules.utils import _pair, _quadruple\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.cm as mpl_color_map\n",
    "import copy\n",
    "\n",
    "print(torch.__version__)\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Weedread(Dataset):\n",
    "    def __init__(self, name, transform=None):\n",
    "        hf = h5py.File(name, 'r')\n",
    "        self.input_images = np.array(hf.get('data'), np.uint8)\n",
    "        self.target_labels = np.array(hf.get('labels')).astype(np.long)\n",
    "        self.transform = transform\n",
    "        hf.close()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.input_images.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        images = self.input_images[idx]\n",
    "        classes = self.target_labels[idx][1]\n",
    "        family =  self.target_labels[idx][0]\n",
    "        if self.transform is not None:\n",
    "            images = self.transform(images)\n",
    "        images = images\n",
    "        \n",
    "        return images, classes, family\n",
    "    \n",
    "class calTech(Dataset):\n",
    "    def __init__(self, name, transform=None):\n",
    "        hf = h5py.File(name, 'r')\n",
    "        self.input_images = np.array(hf.get('features')) / 255.0\n",
    "        self.input_labels = np.array(hf.get('labels')).astype(np.long)\n",
    "        self.transform = transform\n",
    "        hf.close()\n",
    "\n",
    "    def __len__(self):\n",
    "        return (self.input_images.shape[0])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        images = self.input_images[idx]\n",
    "        labels = self.input_labels[idx]\n",
    "        if self.transform is not None:\n",
    "            images = self.transform(images)\n",
    "        return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9145\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8f22328db344391991845cea3093ea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9145), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "INPUT_CHANNEL = 3\n",
    "BATCH_SIZE = 1\n",
    "normalize = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "classes = [str(i) for i in range(102)]\n",
    "\n",
    "data_path = os.path.dirname(os.getcwd()) + \"/data/calTech/\"\n",
    "Train_data = calTech(data_path + \"train.h5\", transform=normalize)\n",
    "Train_dataloader = DataLoader(dataset=Train_data, batch_size = BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(Train_data.__len__())\n",
    "batch = []\n",
    "for train_images, target_class in tqdm(Train_dataloader):\n",
    "    train_images, target_class = train_images.to(device), target_class.to(device)\n",
    "    train_images = train_images.float()\n",
    "    batch.append([train_images, target_class])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "INPUT_CHANNEL = 3\n",
    "BATCH_SIZE = 1\n",
    "normalize = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "imagenet_classes = range(1, 22)\n",
    "\n",
    "data_path = os.path.dirname(os.getcwd()) + \"/data/weed/\"\n",
    "#Train_data = Weedread(data_path + \"train.h5\", transform=normalize)\n",
    "Test_data = Weedread(data_path + \"val.h5\", transform=normalize)\n",
    "\n",
    "#Train_dataloader = DataLoader(dataset=Train_data, batch_size = BATCH_SIZE, shuffle=True)\n",
    "Test_dataloader = DataLoader(dataset=Test_data, batch_size = BATCH_SIZE, shuffle=True)\n",
    "\n",
    "#print(Train_data.__len__())\n",
    "# def get_iterator(mode):\n",
    "#     if mode is True:\n",
    "#         return Train_dataloader\n",
    "#     elif mode is False:\n",
    "#         return Test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatness_Pool2d(nn.Module):\n",
    "    \"\"\" \n",
    "    Args:\n",
    "         kernel_size: size of pooling kernel, int or 2-tuple\n",
    "         stride: pool stride, int or 2-tuple\n",
    "         padding: pool padding, int or 4-tuple (l, r, t, b) as in pytorch F.pad\n",
    "         same: override padding and enforce same padding, boolean\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size=3, stride=1, padding=0, same=False, mode=\"constant\", alpha=0):\n",
    "        super(Flatness_Pool2d, self).__init__()\n",
    "        self.k = _pair(kernel_size)\n",
    "        self.stride = _pair(stride)\n",
    "        self.padding = _quadruple(padding)  # convert to l, r, t, b\n",
    "        self.same = same\n",
    "        self.mode = mode\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def _padding(self, x):\n",
    "        if self.same:\n",
    "            ih, iw = x.size()[2:]\n",
    "            if ih % self.stride[0] == 0:\n",
    "                ph = max(self.k[0] - self.stride[0], 0)\n",
    "            else:\n",
    "                ph = max(self.k[0] - (ih % self.stride[0]), 0)\n",
    "            if iw % self.stride[1] == 0:\n",
    "                pw = max(self.k[1] - self.stride[1], 0)\n",
    "            else:\n",
    "                pw = max(self.k[1] - (iw % self.stride[1]), 0)\n",
    "            pl = pw // 2\n",
    "            pr = pw - pl\n",
    "            pt = ph // 2\n",
    "            pb = ph - pt\n",
    "            padding = (pl, pr, pt, pb)\n",
    "        else:\n",
    "            padding = self.padding\n",
    "        return padding\n",
    "    \n",
    "    def flatness(self, x):\n",
    "        epsilon = 10e-4\n",
    "        x = F.relu(x) + epsilon\n",
    "        n = x.size(-1)\n",
    "        arithmetic_mean = x.mean(dim=-1)\n",
    "        #geometric_mean = x.prod(dim=-1)\n",
    "        #geometric_mean = geometric_mean.pow(1./n)\n",
    "        ln_x = torch.log(x)\n",
    "        geometric_mean = ln_x.mean(dim=-1)\n",
    "        geometric_mean = torch.exp(geometric_mean)\n",
    "        return geometric_mean/arithmetic_mean\n",
    "\n",
    "    def f(self, flatness, alpha):\n",
    "        return 2*alpha*flatness + (1-alpha)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # using existing pytorch functions and tensor ops so that we get autograd, \n",
    "        # would likely be more efficient to implement from scratch at C/Cuda level\n",
    "        x = F.pad(x, self._padding(x), mode=self.mode)\n",
    "        x = x.unfold(2, self.k[0], self.stride[0]).unfold(3, self.k[1], self.stride[1])\n",
    "        x = x.contiguous().view(x.size()[:4] + (-1,))\n",
    "        max_pool = x.max(dim = -1)[0]\n",
    "        flatness = self.flatness(x)\n",
    "        c = self.f(flatness, self.alpha)\n",
    "        out = torch.pow(max_pool, c)\n",
    "        #print(torch.isnan(flatness).sum())\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "class My_Model(nn.Module):\n",
    "    def __init__(self, input_channel=3, num_class=21, alpha=0):\n",
    "        super(My_Model, self).__init__()\n",
    "        self.model_ft = models.resnet18(pretrained=True)\n",
    "        set_parameter_requires_grad(self.model_ft, False)\n",
    "        \n",
    "        #modified pooling\n",
    "        self.model_ft.avgpool = Flatness_Pool2d(kernel_size=7, stride=1, alpha=alpha)\n",
    "        \n",
    "        #change FC\n",
    "        num_ftrs = self.model_ft.fc.in_features\n",
    "        self.model_ft.fc = nn.Linear(num_ftrs, num_class)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Perform the usual forward pass\n",
    "        x = self.model_ft(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/utkuozbulak/pytorch-cnn-visualizations/blob/master/src/gradcam.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "inv_normalize = transforms.Normalize([-0.485/0.229, -0.456/0.224, -0.406/0.225], [1/0.229, 1/0.224, 1/0.225])\n",
    "\n",
    "def format_np_output(np_arr):\n",
    "    \"\"\"\n",
    "        This is a (kind of) bandaid fix to streamline saving procedure.\n",
    "        It converts all the outputs to the same format which is 3xWxH\n",
    "        with using sucecssive if clauses.\n",
    "    Args:\n",
    "        im_as_arr (Numpy array): Matrix of shape 1xWxH or WxH or 3xWxH\n",
    "    \"\"\"\n",
    "    # Phase/Case 1: The np arr only has 2 dimensions\n",
    "    if len(np_arr.shape) == 2:\n",
    "        np_arr = np.expand_dims(np_arr, axis=0)\n",
    "    # Phase/Case 2: Np arr has only 1 channel (assuming first dim is channel)\n",
    "    if np_arr.shape[0] == 1:\n",
    "        np_arr = np.repeat(np_arr, 3, axis=0)\n",
    "    # Phase/Case 3: Np arr is of shape 3xWxH\n",
    "    if np_arr.shape[0] == 3:\n",
    "        np_arr = np_arr.transpose(1, 2, 0)\n",
    "    # Phase/Case 4: NP arr is normalized between 0-1\n",
    "    if np.max(np_arr) <= 1:\n",
    "        np_arr = (np_arr*255).astype(np.uint8)\n",
    "    return np_arr\n",
    "\n",
    "def tensor_to_img(t):\n",
    "    \"\"\"Convert normalized tensor in Cuda to cv2 image\"\"\"\n",
    "    unnormalized = inv_normalize(t)\n",
    "    npimg = format_np_output(unnormalized.cpu().numpy())\n",
    "    pilimage = Image.fromarray(npimg)\n",
    "    return pilimage\n",
    "\n",
    "def save_image(im, path):\n",
    "    \"\"\"\n",
    "        Saves a numpy matrix or PIL image as an image\n",
    "    Args:\n",
    "        im_as_arr (Numpy array): Matrix of shape DxWxH\n",
    "        path (str): Path to the image\n",
    "    \"\"\"\n",
    "    if isinstance(im, (np.ndarray, np.generic)):\n",
    "        im = format_np_output(im)\n",
    "        im = Image.fromarray(im)\n",
    "    im.save(path)\n",
    "    \n",
    "def save_class_activation_images(org_img, activation_map, file_name):\n",
    "    \"\"\"\n",
    "        Saves cam activation map and activation map on the original image\n",
    "    Args:\n",
    "        org_img (PIL img): Original image\n",
    "        activation_map (numpy arr): Activation map (grayscale) 0-255\n",
    "        file_name (str): File name of the exported image\n",
    "    \"\"\"\n",
    "    path_name = 'results0'\n",
    "    if not os.path.exists(path_name):\n",
    "        os.makedirs(path_name)\n",
    "    # Grayscale activation map\n",
    "    heatmap, heatmap_on_image = apply_colormap_on_image(org_img, activation_map, 'hsv')\n",
    "    # Save colored heatmap\n",
    "    #path_to_file = os.path.join(path_name, file_name+'_Cam_Heatmap.png')\n",
    "    #save_image(heatmap, path_to_file)\n",
    "    # Save heatmap on iamge\n",
    "    path_to_file = os.path.join(path_name, file_name+'_Cam_On_Image.png')\n",
    "    save_image(heatmap_on_image, path_to_file)\n",
    "    # SAve grayscale heatmap\n",
    "    #path_to_file = os.path.join(path_name, file_name+'_Cam_Grayscale.png')\n",
    "    #save_image(activation_map, path_to_file)\n",
    "\n",
    "\n",
    "def apply_colormap_on_image(org_im, activation, colormap_name):\n",
    "    \"\"\"\n",
    "        Apply heatmap on image\n",
    "    Args:\n",
    "        org_img (PIL img): Original image\n",
    "        activation_map (numpy arr): Activation map (grayscale) 0-255\n",
    "        colormap_name (str): Name of the colormap\n",
    "    \"\"\"\n",
    "    # Get colormap\n",
    "    color_map = mpl_color_map.get_cmap(colormap_name)\n",
    "    no_trans_heatmap = color_map(activation)\n",
    "    # Change alpha channel in colormap to make sure original image is displayed\n",
    "    heatmap = copy.copy(no_trans_heatmap)\n",
    "    heatmap[:, :, 3] = 0.4\n",
    "    heatmap = Image.fromarray((heatmap*255).astype(np.uint8))\n",
    "    no_trans_heatmap = Image.fromarray((no_trans_heatmap*255).astype(np.uint8))\n",
    "\n",
    "    # Apply heatmap on iamge\n",
    "    heatmap_on_image = Image.new(\"RGBA\", org_im.size)\n",
    "    heatmap_on_image = Image.alpha_composite(heatmap_on_image, org_im.convert('RGBA'))\n",
    "    heatmap_on_image = Image.alpha_composite(heatmap_on_image, heatmap)\n",
    "    return no_trans_heatmap, heatmap_on_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CamExtractor():\n",
    "    \"\"\"\n",
    "        Extracts cam features from the model\n",
    "    \"\"\"\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model.model_ft\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "\n",
    "    def save_gradient(self, grad):\n",
    "        self.gradients = grad\n",
    "\n",
    "    def forward_pass_on_convolutions(self, x):\n",
    "        \"\"\"\n",
    "            Does a forward pass on convolutions, hooks the function at given layer\n",
    "        \"\"\"\n",
    "        conv_output = None\n",
    "        for module_name, module in self.model._modules.items():\n",
    "            if module_name == \"fc\":\n",
    "                x = x.view(x.size(0), -1)\n",
    "            x = module(x)  # Forward\n",
    "            if module_name == self.target_layer:\n",
    "                x.register_hook(self.save_gradient)\n",
    "                conv_output = x  # Save the convolution output on that layer\n",
    "        return conv_output, x\n",
    "\n",
    "    def forward_pass(self, x):\n",
    "        \"\"\"\n",
    "            Does a full forward pass on the model\n",
    "        \"\"\"\n",
    "        # Forward pass on the convolutions\n",
    "        conv_output, x = self.forward_pass_on_convolutions(x)\n",
    "        return conv_output, x\n",
    "\n",
    "\n",
    "class GradCam():\n",
    "    \"\"\"\n",
    "        Produces class activation map\n",
    "    \"\"\"\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        # Define extractor\n",
    "        self.extractor = CamExtractor(self.model, target_layer)\n",
    "\n",
    "    def generate_cam(self, input_image, target_class=None):\n",
    "        # Full forward pass\n",
    "        # conv_output is the output of convolutions at specified layer\n",
    "        # model_output is the final output of the model (1, 1000)\n",
    "        conv_output, model_output = self.extractor.forward_pass(input_image)\n",
    "        if target_class is None:\n",
    "            target_class = np.argmax(model_output.data.numpy())\n",
    "        # Target for backprop\n",
    "        one_hot_output = torch.FloatTensor(1, model_output.size()[-1]).zero_()\n",
    "        one_hot_output[0][target_class] = 1\n",
    "        one_hot_output = one_hot_output.to(device)\n",
    "        # Zero grads\n",
    "        self.model.model_ft.zero_grad()\n",
    "        # Backward pass with specified target\n",
    "        model_output.backward(gradient=one_hot_output, retain_graph=True)\n",
    "        # Get hooked gradients\n",
    "        guided_gradients = self.extractor.gradients.data.cpu().numpy()[0]\n",
    "        # Get convolution outputs\n",
    "        target = conv_output.data.cpu().numpy()[0]\n",
    "        # Get weights from gradients\n",
    "        weights = np.mean(guided_gradients, axis=(1, 2))  # Take averages for each gradient\n",
    "        # Create empty numpy array for cam\n",
    "        cam = np.ones(target.shape[1:], dtype=np.float32)\n",
    "        # Multiply each weight with its conv output and then, sum\n",
    "        for i, w in enumerate(weights):\n",
    "            cam += w * target[i, :, :]\n",
    "       \n",
    "        cam = np.maximum(cam, 0)\n",
    "        cam = (cam - np.min(cam)) / (np.max(cam) - np.min(cam))  # Normalize between 0-1\n",
    "        cam = np.uint8(cam * 255)  # Scale between 0-255 to visualize\n",
    "        cam = np.uint8(Image.fromarray(cam).resize((input_image.shape[2], input_image.shape[3]), Image.ANTIALIAS))/255\n",
    "        return cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My_Model(\n",
      "  (model_ft): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): Flatness_Pool2d()\n",
      "    (fc): Linear(in_features=512, out_features=102, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "train_images, _, = next(iter(Train_dataloader))\n",
    "\n",
    "_model = My_Model(num_class = 102, alpha = -0.1)\n",
    "#summary(_model, input_size= train_images[0].size(), device=\"cpu\")\n",
    "_model = _model.to(device)\n",
    "_model.load_state_dict(torch.load('epochs/ResNet-Flatness-01-CalTech.pt'), strict=False)\n",
    "print(_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:71: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    }
   ],
   "source": [
    "grad_cam = GradCam(_model, target_layer=\"layer4\")\n",
    "\n",
    "i = 1\n",
    "for train_images, target_class in batch:\n",
    "    train_images, target_class = train_images.to(device), target_class.to(device)\n",
    "    train_images = train_images.float()\n",
    "    cam = grad_cam.generate_cam(train_images, target_class)\n",
    "    original_image = tensor_to_img(train_images[0])\n",
    "    save_class_activation_images(original_image, cam, str(i))\n",
    "    if(i == 100):\n",
    "        break\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
